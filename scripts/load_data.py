import os
import sys
import datetime

# ‡∏•‡πâ‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏Ñ‡πâ‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏≠‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡∏´‡∏°‡∏î
if "SPARK_HOME" in os.environ: del os.environ["SPARK_HOME"]
if "PYSPARK_PYTHON" in os.environ: del os.environ["PYSPARK_PYTHON"]

from pyspark.sql import SparkSession
from pyspark.sql import functions as F

def log_status(message):
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{now}] üöÄ {message}")

def clean_airbnb_data(df):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏™‡∏∞‡∏≠‡∏≤‡∏î (‡∏´‡∏ô‡∏π‡πÄ‡∏≠‡∏≤ Logic ‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏™‡∏≠‡∏ô‡∏°‡∏≤‡πÉ‡∏™‡πà‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢)"""
    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
    df_cleaned = df.fillna({'name': 'Unknown', 'price': 0})
    return df_cleaned

def start_spark():
    try:
        # ‡πÉ‡∏ä‡πâ Relative Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå JAR (‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå jars ‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå)
        jar_path = os.path.abspath("./jars/postgresql-42.7.8.jar")
        
        spark = SparkSession.builder \
            .master("local") \
            .appName("Airbnb_Pro_Pipeline") \
            .config("spark.driver.host", "127.0.0.1") \
            .config("spark.driver.bindAddress", "127.0.0.1") \
            .config("spark.jars", jar_path) \
            .getOrCreate()
        return spark
    except Exception as e:
        log_status(f"üí• ‡∏™‡∏£‡πâ‡∏≤‡∏á Spark ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÄ‡∏û‡∏£‡∏≤‡∏∞: {e}")
        return None

# --- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏£‡∏±‡∏ô‡∏á‡∏≤‡∏ô ---
spark = start_spark()

if spark:
    log_status("‡∏´‡∏±‡∏ß‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏ô‡∏á‡∏≤‡∏ô Spark ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏•‡∏∏‡∏¢‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡πÇ‡∏õ‡∏£‡πÅ‡∏•‡πâ‡∏ß!")
    files = ['listings', 'neighbourhoods', 'reviews']
    
    # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    db_url = "jdbc:postgresql://localhost:5432/airbnb_raw"
    db_properties = {
        "user": "admin",
        "password": "password123", 
        "driver": "org.postgresql.Driver"
    }

    for file_name in files:
        # ‡πÉ‡∏ä‡πâ Relative Path (./) ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏ó‡∏∏‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á
        path = f"./dataset/raw/{file_name}.csv"
        log_status(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå: {file_name}")
        
        try:
            df = spark.read.csv(path, header=True, inferSchema=True)
            
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå listings ‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏•‡∏µ‡∏ô‡∏Å‡πà‡∏≠‡∏ô
            if file_name == 'listings':
                log_status("‚ú® ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Listings...")
                df = clean_airbnb_data(df)
            
            log_status(f"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á {file_name} ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Postgres...")
            df.write.jdbc(url=db_url, table=file_name, mode="overwrite", properties=db_properties)
            log_status(f"‚úÖ {file_name} ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢!")
            
        except Exception as e:
            log_status(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå {file_name}: {e}")

else:
    log_status("Spark ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö JAVA_HOME ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ô‡∏∞‡∏•‡∏π‡∏Å")